{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad de investigación sobre JAX\n",
        "# Autor: Germán García Estévez"
      ],
      "metadata": {
        "id": "wE_luKyDRf3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Qué es JAX y cuáles son sus principales características.\n",
        "\n",
        "JAX es una biblioteca de Python desarrollada por Google que destaca por su capacidad de diferenciación automática **(autograd)** y su integración con aceleradores **(GPU/TPU)** para realizar cálculos de manera muy eficiente. Algunas de sus principales características son:\n",
        "\n",
        "* **Autograd avanzado:** permite calcular gradientes de manera fácil y rápida, incluso con funciones y estructuras complejas.\n",
        "\n",
        "* **Compilación just-in-time (JIT) con XLA:** optimiza el código y lo acelera significativamente al compilarlo para CPU, GPU o TPU.\n",
        "\n",
        "* **Transformaciones funcionales:** provee herramientas como `vmap` (vectorización automática) y `pmap` (paralelización en múltiples dispositivos), facilitando el escalado en grandes volúmenes de datos.\n",
        "\n",
        "* **Enfoque funcional:** JAX promueve un estilo de programación inmutable y funcional, lo que ayuda a evitar errores comunes en entornos de investigación.\n",
        "\n",
        "En resumen, es una herramienta muy útil para el desarrollo de proyectos de ML e investigación científica que requieran cálculos rápidos y precisos."
      ],
      "metadata": {
        "id": "Sxz4Jnu7Roc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apw7olegRBCr",
        "outputId": "703992d0-e587-4710-9c7c-840f8348f76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f(x) =  16.0\n",
            "Gradiente de f en x =  8.0\n",
            "Resultado de f(x) compilado con JIT =  16.0\n",
            "Aplicando la función a un array con vmap: [ 4  9 16]\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo del punto 1\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, grad, vmap\n",
        "\n",
        "# Definimos una función sencilla\n",
        "def f(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Obtenemos su gradiente con respecto a x\n",
        "f_grad = grad(f)\n",
        "\n",
        "# Compilamos la función con JIT para acelerar su ejecución\n",
        "f_jit = jit(f)\n",
        "\n",
        "# Ejemplo de uso\n",
        "x = 3.0\n",
        "print(\"f(x) = \", f(x))\n",
        "print(\"Gradiente de f en x = \", f_grad(x))\n",
        "print(\"Resultado de f(x) compilado con JIT = \", f_jit(x))\n",
        "\n",
        "# Vectorizamos la función con vmap para aplicarla a un array\n",
        "xs = jnp.array([1, 2, 3])\n",
        "f_vmap = vmap(f)\n",
        "print(\"Aplicando la función a un array con vmap:\", f_vmap(xs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `grad(f)`: obtiene de forma automática el gradiente de `f`.\n",
        "\n",
        "* `jit(f)`: compila la función para acelerar la ejecución utilizando **XLA**.\n",
        "\n",
        "* `vmap(f)`: permite aplicar la función `f` de forma vectorizada a un conjunto de datos sin necesidad de escribir bucles explícitos."
      ],
      "metadata": {
        "id": "Hd8f-lgYTdHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Comparación de JAX con TensorFlow y PyTorch.\n",
        "\n",
        "### 1. Estilo de programación:\n",
        "\n",
        "* **JAX**: se basa en un estilo *funcional*. Esto significa que trabaja con funciones \"puras\" (sin efectos secundarios) y usa transformaciones como `grad` (para derivadas automáticas), `vmap` (para vectorización) y `pmap` (para procesamiento en paralelo).\n",
        "\n",
        "* **TensorFlow y PyTorch**: usan un enfoque más *imperativo* o de \"programación directa\". En PyTorch, escribes el código y \"mágicamente\" se rastrean los gradientes; en TensorFlow, si usas `tf.GradientTape`, también obtienes gradientes fácilmente, pero en general han tardado más en simplificar su uso.\n",
        "\n",
        "### 2. Diferenciación automática `(autograd)`:\n",
        "\n",
        "* **JAX**: con solo escribir funciones en Python usando `jax.numpy`, puedes obtener gradientes con `grad`. Es muy transparente y directo.\n",
        "\n",
        "* **TensorFlow y PyTorch**: hacen algo similar, pero su configuración a veces requiere un poco más de trabajo o configuración (especialmente TensorFlow).\n",
        "\n",
        "### 3. Rendimiento y aceleración:\n",
        "\n",
        "* **JAX**: usa la compilación just-in-time (JIT) con XLA, lo que puede lograr mucha velocidad al procesar en CPU, GPU o TPU.\n",
        "\n",
        "* **TensorFlow**: también puede usar XLA y es muy popular para producción, también tiene soporte oficial de Google para TPU, etc.\n",
        "\n",
        "* **PyTorch**: inicialmente se centraba en GPU y CPU, pero ahora también tiene soporte para TPU (aunque más reciente).\n",
        "\n",
        "### 4. Ecosistema:\n",
        "\n",
        "* **JAX**: es relativamente nuevo, su ecosistema (bibliotecas, tutoriales, proyectos) está creciendo rápido, pero aún no es tan grande como el de PyTorch o TensorFlow.\n",
        "\n",
        "* **TensorFlow**: tiene un ecosistema industrial muy grande (TensorFlow Serving, TensorFlow Lite, etc.) ideal para despliegues en empresas.\n",
        "\n",
        "* **PyTorch**: muy popular en investigación gracias a su facilidad de uso y abundancia de ejemplos, modelos preentrenados y librerías de terceros.\n",
        "\n",
        "### 5. Aprendizaje y estilo:\n",
        "\n",
        "* **JAX**: requiere acostumbrarse al estilo funcional (no mutar variables, trabajar con transformaciones como `grad`, `vmap`, etc.).\n",
        "\n",
        "* **TensorFlow**: ha evolucionado de un enfoque más complejo (gráficos estáticos en TF1.x) a uno más amigable (ejecución \"eager\" en TF2.x).\n",
        "\n",
        "* **PyTorch**: es muy cercano al \"Python puro\", lo que lo hace muy intuitivo para la mayoría de personas que empiezan a programar en *Deep Learning*.\n",
        "\n",
        "**A modo de resumen:** si lo que buscas es un enfoque muy flexible, con gran eficiencia en CPU/GPU/TPU y te gusta el estilo funcional, *JAX* es una excelente opción.\n",
        "\n",
        "Pero si quieres algo más tradicional, con muchísimo soporte en la comunidad y fácil de entender desde el principio, *PyTorch* es muy buena alternativa.\n",
        "\n",
        "Y si trabajas en proyectos de gran escala a nivel empresarial, *TensorFlow* ofrece un ecosistema muy completo para desplegar y mantener modelos en producción."
      ],
      "metadata": {
        "id": "uaIPmoSeTs65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo del punto 2. Voy a usar la misma función del ejemplo del punto 1.\n",
        "# Ejemplo con JAX\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "import timeit\n",
        "\n",
        "# Definimos la función f(x)\n",
        "def f_jax(x):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "# Obtenemos su gradiente\n",
        "df_jax = grad(f_jax)\n",
        "\n",
        "# Iniciamos el temporizador\n",
        "inicio = timeit.default_timer()\n",
        "\n",
        "x_jax = 3.0\n",
        "valor_funcion = f_jax(x_jax)\n",
        "valor_gradiente = df_jax(x_jax)\n",
        "\n",
        "# Terminamos el temporizador\n",
        "fin = timeit.default_timer()\n",
        "\n",
        "# Mostramos resultados\n",
        "print(\"JAX - f(3.0):\", valor_funcion)           # Valor de la función\n",
        "print(\"JAX - Gradiente en 3.0:\", valor_gradiente)  # Valor del gradiente\n",
        "print(\"Tiempo de ejecución (s):\", fin - inicio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0eEs_VLTqYA",
        "outputId": "1e2dd715-2bfc-46e2-e7e7-51c2b17b2eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX - f(3.0): 16.0\n",
            "JAX - Gradiente en 3.0: 8.0\n",
            "Tiempo de ejecución (s): 0.010217507999641384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo con TensorFlow\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# Iniciamos el temporizador\n",
        "inicio = time.time()\n",
        "\n",
        "x_tf = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y_tf = x_tf**2 + 2*x_tf + 1  # f(x)\n",
        "\n",
        "grad_tf = tape.gradient(y_tf, x_tf)\n",
        "\n",
        "# Terminamos el temporizador\n",
        "fin = time.time()\n",
        "\n",
        "# Mostramos resultados\n",
        "print(\"\\nTensorFlow - f(3.0):\", y_tf.numpy())       # Valor de la función\n",
        "print(\"TensorFlow - Gradiente en 3.0:\", grad_tf.numpy())  # Valor del gradiente\n",
        "print(\"Tiempo de ejecución (s):\", fin - inicio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scpi2VbtXhd4",
        "outputId": "34100995-c127-4706-cbe1-22dea6bbd6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TensorFlow - f(3.0): 16.0\n",
            "TensorFlow - Gradiente en 3.0: 8.0\n",
            "Tiempo de ejecución (s): 0.006559133529663086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `GradientTape`: TensorFlow usa esta \"cinta\" para grabar las operaciones y luego calcular gradientes.\n",
        "\n",
        "* Ecosistema robusto: ideal para producción, con herramientas como TensorFlow Serving o TensorFlow Lite."
      ],
      "metadata": {
        "id": "UezM7-1EXsht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo con PyTorch\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Iniciamos el temporizador\n",
        "inicio = time.time()\n",
        "\n",
        "x_torch = torch.tensor(3.0, requires_grad=True)\n",
        "y_torch = x_torch**2 + 2*x_torch + 1  # f(x)\n",
        "y_torch.backward()  # Calcula el gradiente\n",
        "\n",
        "# Terminamos el temporizador\n",
        "fin = time.time()\n",
        "\n",
        "# Mostramos resultados\n",
        "print(\"\\nPyTorch - f(3.0):\", y_torch.item())         # Valor de la función\n",
        "print(\"PyTorch - Gradiente en 3.0:\", x_torch.grad.item())  # Valor del gradiente\n",
        "print(\"Tiempo de ejecución (s):\", fin - inicio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZlLZJjiXxDw",
        "outputId": "b52cdabb-6e1e-408a-8fab-fa26b9cbac61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PyTorch - f(3.0): 16.0\n",
            "PyTorch - Gradiente en 3.0: 8.0\n",
            "Tiempo de ejecución (s): 0.12060165405273438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Computational graph dinámico: PyTorch sigue cada operación en tiempo real y permite calcular gradientes de manera intuitiva."
      ],
      "metadata": {
        "id": "KsAOqrAYX4Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Ecosistema: librerías implementadas sobre JAX y otras herramientas que se integran bien con esta tecnología.\n",
        "\n",
        "Dentro del ecosistema de JAX han surgido múltiples librerías y herramientas enfocadas en diferentes áreas (aprendizaje profundo, estadística bayesiana, optimización avanzada, etc.):\n",
        "\n",
        "### **Flax:**\n",
        "Librería oficial de *Google* para construir redes neuronales en JAX. Enfoque modular y flexible para desarrollar y entrenar modelos de Deep Learning.\n",
        "\n",
        "### **Haiku:**\n",
        "Desarrollada por *DeepMind*, también orientada a redes neuronales en JAX. Utiliza un estilo de programación más cercano a PyTorch (definición de módulos/clases).\n",
        "\n",
        "### **Optax:**\n",
        "Biblioteca (de *DeepMind*) de algoritmos de optimización, pensada para integrarse fácilmente con Flax, Haiku u otras librerías JAX. Ofrece optimizadores clásicos (`SGD`, `Adam`, `RMSProp`) y métodos más avanzados (como `Adafactor`, `LAMB`).\n",
        "\n",
        "### **Chex:**\n",
        "Conjunto de utilidades para depurar y probar (testing) el código de JAX. Incluye herramientas para validar shapes de tensores y verificar gradientes, entre otras.\n",
        "\n",
        "### **RLax:**\n",
        "Librería específica para Reinforcement Learning (RL) desarrollada por *DeepMind*. Ofrece implementaciones de pérdidas (`losses`) y rutinas clásicas de RL (`Q-learning`, `Policy Gradients`).\n",
        "\n",
        "### **NumPyro y BlackJAX:**\n",
        "Dos librerías para estadística bayesiana y MCMC en JAX. Facilitan la construcción de modelos probabilísticos y la realización de inferencia con métodos como `Hamiltonian Monte Carlo`.\n",
        "\n",
        "### **Brax:**\n",
        "Motor de física en JAX para simular entornos con sistemas articulados (robótica, simulaciones físicas simples). Muy orientado a experimentación en RL y optimización.\n",
        "\n",
        "### **Jraph:**\n",
        "Diseñada para trabajar con grafos en JAX. Facilita la construcción de Graph Neural Networks (`GNNs`).\n",
        "\n",
        "### **Integración con Hugging Face:**\n",
        "Algunos modelos de Hugging Face (principalmente en `NLP`) tienen soporte para JAX, permitiendo entrenar y servir modelos usando esta tecnología."
      ],
      "metadata": {
        "id": "cG82gXy2YAAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En conjunto, estas librerías aprovechan las transformaciones de JAX (como `grad`, `jit`, `vmap`) y su enfoque funcional para facilitar tareas de investigación y desarrollo.\n",
        "\n",
        "Además, JAX se integra bien con el ecosistema de Python (`NumPy`, `SciPy`, `Pandas`, etc.), lo que ayuda a la creación de flujos de trabajo complejos en Machine Learning e investigación científica."
      ],
      "metadata": {
        "id": "r3013NM2aUpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo del punto 3\n",
        "# Ejemplo de integración de Flax y Optax con JAX\n",
        "# Se entrena un modelo MLP simple para aproximar una función en datos sintéticos\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "\n",
        "# 1. Definimos un modelo sencillo (MLP) con Flax\n",
        "class SimpleMLP(nn.Module):\n",
        "    features: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(self.features)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(1)(x)  # Capa de salida con 1 unidad\n",
        "        return x\n",
        "\n",
        "# 2. Función de pérdida (mean squared error)\n",
        "def mse_loss(params, x, y):\n",
        "    preds = model.apply({'params': params}, x)  # forward pass\n",
        "    return jnp.mean((preds - y) ** 2)\n",
        "\n",
        "# 3. Envolvemos la pérdida y la actualización en funciones JIT\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, x, y):\n",
        "    # Calculamos gradientes\n",
        "    grads = jax.grad(mse_loss)(params, x, y)\n",
        "    # Obtenemos las actualizaciones con Optax\n",
        "    updates, opt_state = tx.update(grads, opt_state, params)\n",
        "    # Aplicamos las actualizaciones a los parámetros\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state\n",
        "\n",
        "# Creamos una instancia de la red\n",
        "model = SimpleMLP(features=16)\n",
        "\n",
        "# Generamos datos aleatorios (x_dummy, y_dummy) para el entrenamiento de ejemplo\n",
        "key = jax.random.PRNGKey(0)\n",
        "x_dummy = jax.random.normal(key, (10, 5))  # 10 muestras, 5 características\n",
        "y_dummy = jax.random.normal(key, (10, 1))  # Etiquetas correspondientes\n",
        "\n",
        "# Inicializamos parámetros y el optimizador\n",
        "params = model.init(key, x_dummy)['params']\n",
        "tx = optax.adam(learning_rate=1e-2)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "# Realizamos una sola pasada de entrenamiento\n",
        "params, opt_state = train_step(params, opt_state, x_dummy, y_dummy)\n",
        "\n",
        "# Mostramos el valor de la función de pérdida después de una actualización\n",
        "loss_value = mse_loss(params, x_dummy, y_dummy)\n",
        "print(\"Loss tras un paso de entrenamiento:\", loss_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvB4wD1TYIQj",
        "outputId": "4ccd8723-6467-4a96-f932-4090b27ab68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss tras un paso de entrenamiento: 0.59859973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Ejemplo práctico\n",
        "\n",
        "### Clasificador de Imágenes CIFAR-10 con JAX y Streamlit:\n",
        "https://cifar-10-clasificador.streamlit.app/\n",
        "\n",
        "El código usado para el entrenamiento del modelo y de la descarga del dataset está subido en el GitHub, con el nombre `train_robust_model.py`. No lo incluyo aquí porque tarda bastante en ejecutarse, y en Colab ni se llega a terminar.\n",
        "\n",
        "No he usado el dataset entero, sino una parte de él.\n",
        "\n",
        "Se adjunta una captura de pantalla de las últimas líneas de ejecución del archivo, donde se muestra la precisión del modelo.\n",
        "\n",
        "![Precisión](https://drive.google.com/uc?id=1toxjtFLwpAI5YoVCTy3gftf0EVhkCIb2)\n"
      ],
      "metadata": {
        "id": "b6XAVGcGbGKm"
      }
    }
  ]
}